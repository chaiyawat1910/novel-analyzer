import streamlit as st
import pandas as pd
import altair as alt
from pythainlp import word_tokenize
from pythainlp.util import isthai
from pythainlp.tag import NER
from collections import Counter
import graphviz
import io

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö ---
st.set_page_config(page_title="Pro Novel Analyst AI", page_icon="ü§ñ", layout="wide")

# --- 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ (Session State) ---
# ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏´‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à
if 'analyzed' not in st.session_state:
    st.session_state.analyzed = False
if 'result_data' not in st.session_state:
    st.session_state.result_data = {}

st.title("ü§ñ Pro Novel Analyst: ‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞")

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
col_left, col_right = st.columns([1, 1])

with col_left:
    st.subheader("1. ‡πÉ‡∏™‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢")
    input_method = st.radio("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:", ["üìÇ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (.txt)", "‚úçÔ∏è ‡∏ß‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏≠‡∏á"])
    
    novel_text = ""
    
    if input_method == "üìÇ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (.txt)":
        uploaded_file = st.file_uploader("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì", type=['txt'])
        if uploaded_file is not None:
            stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
            novel_text = stringio.read()
            st.success(f"‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß {len(novel_text):,} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)")
            
    else:
        novel_text = st.text_area("‡∏ß‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà:", height=300)

with col_right:
    st.subheader("2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå")
    manual_chars = st.text_area("‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£‡πÄ‡∏≠‡∏á (‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏∏‡∏•‡∏†‡∏≤‡∏Ñ)", placeholder="‡πÄ‡∏ä‡πà‡∏ô: ‡∏™‡∏°‡∏ä‡∏≤‡∏¢, ‡∏™‡∏°‡∏´‡∏ç‡∏¥‡∏á", height=100)
    
    # ‡∏õ‡∏∏‡πà‡∏°‡∏Å‡∏î‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
    if st.button("üöÄ ‡∏™‡∏±‡πà‡∏á AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏ô‡∏µ‡πâ!", type="primary", use_container_width=True):
        if novel_text:
            st.session_state.analyzed = True
            st.rerun() # ‡∏™‡∏±‡πà‡∏á‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        else:
            st.error("‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö")

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô AI (NER Engine) ---
@st.cache_resource
def load_ner_engine():
    return NER("thainer")

def extract_entities(text):
    ner = load_ner_engine()
    # ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ñ‡πâ‡∏≤‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏£‡∏≠‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô 5 ‡∏ô‡∏≤‡∏ó‡∏µ)
    # ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏´‡∏ß‡∏Å‡πá‡πÄ‡∏≠‡∏≤ limit ‡∏≠‡∏≠‡∏Å‡πÑ‡∏î‡πâ
    processed_text = text[:100000] if len(text) > 100000 else text 
    tags = ner.tag(processed_text)
    
    entities = {
        "PERSON": [],
        "LOCATION": [],
        "DATE": [],
        "TIME": []
    }
    
    # ‡∏Ñ‡∏≥‡∏™‡∏£‡∏£‡∏û‡∏ô‡∏≤‡∏°‡∏ó‡∏µ‡πà AI ‡∏°‡∏±‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ú‡∏¥‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô (Blacklist)
    blacklist_names = [
        "‡πÄ‡∏Ç‡∏≤", "‡πÄ‡∏ò‡∏≠", "‡∏°‡∏±‡∏ô", "‡∏â‡∏±‡∏ô", "‡∏ú‡∏°", "‡∏Å‡∏π", "‡∏°‡∏∂‡∏á", "‡∏Ç‡πâ‡∏≤", "‡πÄ‡∏≠‡πá‡∏á", "‡πÄ‡∏£‡∏≤", "‡∏û‡∏ß‡∏Å‡πÄ‡∏£‡∏≤",
        "‡∏û‡∏µ‡πà", "‡∏ô‡πâ‡∏≠‡∏á", "‡∏•‡∏∏‡∏á", "‡∏õ‡πâ‡∏≤", "‡∏ô‡πâ‡∏≤", "‡∏≠‡∏≤", "‡∏û‡πà‡∏≠", "‡πÅ‡∏°‡πà", "‡∏õ‡∏π‡πà", "‡∏¢‡πà‡∏≤", "‡∏ï‡∏≤", "‡∏¢‡∏≤‡∏¢",
        "‡∏Ñ‡∏∏‡∏ì", "‡∏ó‡πà‡∏≤‡∏ô", "‡πÅ‡∏Å", "‡πÉ‡∏Ñ‡∏£", "‡∏ô‡∏≤‡∏á", "‡∏ô‡∏≤‡∏¢", "‡πÄ‡∏î‡πá‡∏Å", "‡∏ú‡∏π‡πâ‡∏ä‡∏≤‡∏¢", "‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á", "‡∏Ñ‡∏ô",
        "‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á", "‡∏ö‡πà‡∏≤‡∏ß", "‡∏ù‡πà‡∏≤‡∏ö‡∏≤‡∏ó", "‡∏û‡∏£‡∏∞‡∏≠‡∏á‡∏Ñ‡πå", "‡∏´‡∏°‡∏≠", "‡∏Ñ‡∏£‡∏π", "‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå"
    ]

    for item in tags:
        if len(item) == 3:
            word, pos, tag = item
        elif len(item) == 2:
            word, tag = item
        else:
            continue 
            
        word_clean = word.strip()
        
        if "PERSON" in tag:
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô 1 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Blacklist
            if len(word_clean) > 1 and word_clean not in blacklist_names:
                entities["PERSON"].append(word_clean)
        elif "LOCATION" in tag:
            entities["LOCATION"].append(word_clean)
        elif "DATE" in tag:
            entities["DATE"].append(word_clean)
        elif "TIME" in tag:
            entities["TIME"].append(word_clean)
            
    return entities

def analyze_sentiment(words):
    pos_words = ["‡∏£‡∏±‡∏Å", "‡∏î‡∏µ", "‡∏™‡∏∏‡∏Ç", "‡∏™‡∏ß‡∏¢", "‡∏¢‡∏¥‡πâ‡∏°", "‡∏´‡∏±‡∏ß‡πÄ‡∏£‡∏≤‡∏∞", "‡∏ä‡∏≠‡∏ö", "‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô", "‡∏´‡∏ß‡∏≤‡∏ô", "‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô", "‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à", "‡∏£‡∏≠‡∏î", "‡∏ä‡∏ô‡∏∞"]
    neg_words = ["‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î", "‡∏ï‡∏≤‡∏¢", "‡∏Ü‡πà‡∏≤", "‡πÄ‡∏•‡∏ß", "‡∏£‡πâ‡∏≠‡∏á‡πÑ‡∏´‡πâ", "‡πÄ‡∏à‡πá‡∏ö", "‡πÇ‡∏Å‡∏£‡∏ò", "‡πÄ‡∏®‡∏£‡πâ‡∏≤", "‡∏ó‡∏£‡∏°‡∏≤‡∏ô", "‡∏Å‡∏•‡∏±‡∏ß", "‡∏°‡∏∑‡∏î‡∏°‡∏ô", "‡πÅ‡∏û‡πâ", "‡πÄ‡∏à‡πá‡∏ö‡∏õ‡∏ß‡∏î"]
    score = 0
    if len(words) > 0:
        pos_cnt = sum(1 for w in words if w in pos_words)
        neg_cnt = sum(1 for w in words if w in neg_words)
        score = pos_cnt - neg_cnt
    return score

# --- ‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• (‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ analyzed ‡πÄ‡∏õ‡πá‡∏ô True) ---
if st.session_state.analyzed and novel_text:
    
    st.divider()
    
    with st.spinner('ü§ñ AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô... (‡∏Å‡∏£‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥ + ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü)'):
        
        # 1. ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥
        raw_words = word_tokenize(novel_text, engine="newmm")
        words = [w for w in raw_words if w.strip() != "" and isthai(w)]
        
        # 2. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å AI ‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠
        found_entities = extract_entities(novel_text)
        
        # 3. ‡∏£‡∏ß‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥ (‡πÉ‡∏ä‡πâ set ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô)
        auto_chars = list(set(found_entities["PERSON"])) 
        user_chars = [c.strip() for c in manual_chars.split(",") if c.strip() != ""]
        
        # ‡∏£‡∏ß‡∏° + ‡∏ï‡∏±‡∏î‡∏ã‡πâ‡∏≥‡∏≠‡∏µ‡∏Å‡∏£‡∏≠‡∏ö
        final_char_list = list(set(auto_chars + user_chars))
        final_char_list.sort() # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£

        st.success(f"‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏û‡∏ö‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£ (Unique) ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(final_char_list)} ‡∏Ñ‡∏ô")

        # --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Tabs ---
        tab1, tab2, tab3, tab4, tab5 = st.tabs(["üó∫Ô∏è ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà AI ‡∏û‡∏ö

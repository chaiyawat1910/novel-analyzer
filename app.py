import streamlit as st
import pandas as pd
import altair as alt
from pythainlp import word_tokenize
from pythainlp.util import isthai
from pythainlp.tag import NER
from collections import Counter
import graphviz
import io

# --- à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¸«à¸™à¹‰à¸²à¹€à¸§à¹‡à¸š ---
st.set_page_config(page_title="Pro Novel Analyst AI", page_icon="ğŸ¤–", layout="wide")

st.title("ğŸ¤– Pro Novel Analyst: à¸£à¸°à¸šà¸šà¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸™à¸´à¸¢à¸²à¸¢à¸­à¸±à¸ˆà¸‰à¸£à¸´à¸¢à¸°")
st.info("à¸­à¸±à¸›à¹€à¸à¸£à¸”à¹ƒà¸«à¸¡à¹ˆ! à¸£à¸­à¸‡à¸£à¸±à¸šà¸à¸²à¸£à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ à¹à¸¥à¸°à¸„à¹‰à¸™à¸«à¸²à¸•à¸±à¸§à¸¥à¸°à¸„à¸£/à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆà¹ƒà¸«à¹‰à¹€à¸­à¸‡à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´")

# --- à¸ªà¹ˆà¸§à¸™à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ (à¸­à¸±à¸›à¹€à¸à¸£à¸”à¹ƒà¸«à¸¡à¹ˆ) ---
col_left, col_right = st.columns([1, 1])

with col_left:
    st.subheader("1. à¹ƒà¸ªà¹ˆà¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸™à¸´à¸¢à¸²à¸¢")
    # Tab à¹€à¸¥à¸·à¸­à¸à¸§à¸´à¸˜à¸µà¸à¸²à¸£à¹ƒà¸ªà¹ˆà¸‚à¹‰à¸­à¸¡à¸¹à¸¥
    input_method = st.radio("à¹€à¸¥à¸·à¸­à¸à¸§à¸´à¸˜à¸µà¸™à¸³à¹€à¸‚à¹‰à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥:", ["ğŸ“‚ à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ (.txt)", "âœï¸ à¸§à¸²à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹€à¸­à¸‡"])
    
    novel_text = ""
    
    if input_method == "ğŸ“‚ à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ (.txt)":
        uploaded_file = st.file_uploader("à¹€à¸¥à¸·à¸­à¸à¹„à¸Ÿà¸¥à¹Œà¸™à¸´à¸¢à¸²à¸¢à¸‚à¸­à¸‡à¸„à¸¸à¸“", type=['txt'])
        if uploaded_file is not None:
            # à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œà¹à¸¥à¸°à¹à¸›à¸¥à¸‡à¹€à¸›à¹‡à¸™à¸•à¸±à¸§à¸«à¸™à¸±à¸‡à¸ªà¸·à¸­
            stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
            novel_text = stringio.read()
            st.success(f"âœ… à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œà¸ªà¸³à¹€à¸£à¹‡à¸ˆ! (à¸„à¸§à¸²à¸¡à¸¢à¸²à¸§ {len(novel_text)} à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£)")
            
    else:
        novel_text = st.text_area("à¸§à¸²à¸‡à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸™à¸´à¸¢à¸²à¸¢à¸—à¸µà¹ˆà¸™à¸µà¹ˆ:", height=300)

with col_right:
    st.subheader("2. à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ")
    st.write("à¸£à¸°à¸šà¸šà¸ˆà¸°à¹ƒà¸Šà¹‰ AI à¸„à¹‰à¸™à¸«à¸²à¸Šà¸·à¹ˆà¸­à¸•à¸±à¸§à¸¥à¸°à¸„à¸£à¹ƒà¸«à¹‰à¹€à¸­à¸‡ à¹à¸•à¹ˆà¸–à¹‰à¸²à¸„à¸¸à¸“à¸•à¹‰à¸­à¸‡à¸à¸²à¸£à¹€à¸à¸´à¹ˆà¸¡à¸Šà¸·à¹ˆà¸­à¹€à¸‰à¸à¸²à¸°à¹€à¸ˆà¸²à¸°à¸ˆà¸‡ à¸ªà¸²à¸¡à¸²à¸£à¸–à¸à¸´à¸¡à¸à¹Œà¹€à¸à¸´à¹ˆà¸¡à¹„à¸”à¹‰à¸”à¹‰à¸²à¸™à¸¥à¹ˆà¸²à¸‡")
    
    # à¸£à¸±à¸šà¸Šà¸·à¹ˆà¸­à¹€à¸à¸´à¹ˆà¸¡à¹€à¸•à¸´à¸¡
    manual_chars = st.text_area("à¹€à¸à¸´à¹ˆà¸¡à¸Šà¸·à¹ˆà¸­à¸•à¸±à¸§à¸¥à¸°à¸„à¸£à¹€à¸­à¸‡ (à¸„à¸±à¹ˆà¸™à¸”à¹‰à¸§à¸¢à¸ˆà¸¸à¸¥à¸ à¸²à¸„ , )", placeholder="à¹€à¸Šà¹ˆà¸™: à¸ªà¸¡à¸Šà¸²à¸¢, à¸ªà¸¡à¸«à¸à¸´à¸‡ (à¸£à¸°à¸šà¸šà¸ˆà¸°à¸£à¸§à¸¡à¸à¸±à¸šà¸—à¸µà¹ˆ AI à¸«à¸²à¹€à¸ˆà¸­)", height=100)
    
    # à¸›à¸¸à¹ˆà¸¡à¸à¸”
    analyze_btn = st.button("ğŸš€ à¸ªà¸±à¹ˆà¸‡ AI à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸”à¸µà¹‹à¸¢à¸§à¸™à¸µà¹‰!", type="primary", use_container_width=True)

# --- à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™ AI (NER Engine) ---
@st.cache_resource # à¹€à¸à¹‡à¸šà¹à¸„à¸Šà¹‚à¸¡à¹€à¸”à¸¥à¹„à¸§à¹‰ à¸ˆà¸°à¹„à¸”à¹‰à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹‚à¸«à¸¥à¸”à¹ƒà¸«à¸¡à¹ˆà¸—à¸¸à¸à¸„à¸£à¸±à¹‰à¸‡à¹ƒà¸«à¹‰à¹€à¸ªà¸µà¸¢à¹€à¸§à¸¥à¸²
def load_ner_engine():
    return NER("thainer")

def extract_entities(text):
    ner = load_ner_engine()
    # tag à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œà¸ˆà¸°à¹€à¸›à¹‡à¸™ list à¸‚à¸­à¸‡ (à¸„à¸³, à¸Šà¸™à¸´à¸”à¸„à¸³, à¸Šà¸™à¸´à¸” Entity)
    tags = ner.tag(text)
    
    entities = {
        "PERSON": [],
        "LOCATION": [],
        "DATE": [],
        "TIME": []
    }
    
    for word, pos, tag in tags:
        if tag == "B-PERSON" or tag == "I-PERSON":
            entities["PERSON"].append(word)
        elif tag == "B-LOCATION" or tag == "I-LOCATION":
            entities["LOCATION"].append(word)
        elif tag == "B-DATE" or tag == "I-DATE":
            entities["DATE"].append(word)
        elif tag == "B-TIME" or tag == "I-TIME":
            entities["TIME"].append(word)
            
    return entities

def analyze_sentiment(words):
    pos_words = ["à¸£à¸±à¸", "à¸”à¸µ", "à¸ªà¸¸à¸‚", "à¸ªà¸§à¸¢", "à¸¢à¸´à¹‰à¸¡", "à¸«à¸±à¸§à¹€à¸£à¸²à¸°", "à¸Šà¸­à¸š", "à¸­à¸šà¸­à¸¸à¹ˆà¸™", "à¸«à¸§à¸²à¸™", "à¸•à¸·à¹ˆà¸™à¹€à¸•à¹‰à¸™", "à¸ªà¸³à¹€à¸£à¹‡à¸ˆ", "à¸£à¸­à¸”", "à¸Šà¸™à¸°"]
    neg_words = ["à¹€à¸à¸¥à¸µà¸¢à¸”", "à¸•à¸²à¸¢", "à¸†à¹ˆà¸²", "à¹€à¸¥à¸§", "à¸£à¹‰à¸­à¸‡à¹„à¸«à¹‰", "à¹€à¸ˆà¹‡à¸š", "à¹‚à¸à¸£à¸˜", "à¹€à¸¨à¸£à¹‰à¸²", "à¸—à¸£à¸¡à¸²à¸™", "à¸à¸¥à¸±à¸§", "à¸¡à¸·à¸”à¸¡à¸™", "à¹à¸à¹‰", "à¹€à¸ˆà¹‡à¸šà¸›à¸§à¸”"]
    score = 0
    if len(words) > 0:
        pos_cnt = sum(1 for w in words if w in pos_words)
        neg_cnt = sum(1 for w in words if w in neg_words)
        score = pos_cnt - neg_cnt
    return score

# --- à¹€à¸£à¸´à¹ˆà¸¡à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸¡à¸·à¹ˆà¸­à¸à¸”à¸›à¸¸à¹ˆà¸¡ ---
if analyze_btn and novel_text:
    with st.spinner('ğŸ¤– AI à¸à¸³à¸¥à¸±à¸‡à¸—à¸³à¸‡à¸²à¸™... (à¸­à¹ˆà¸²à¸™à¸™à¸´à¸¢à¸²à¸¢ > à¸«à¸²à¸•à¸±à¸§à¸¥à¸°à¸„à¸£ > à¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿ)'):
        
        # 1. à¸•à¸±à¸”à¸„à¸³ (Tokenization)
        raw_words = word_tokenize(novel_text, engine="newmm")
        words = [w for w in raw_words if w.strip() != "" and isthai(w)]
        
        # 2. à¹€à¸£à¸µà¸¢à¸ AI à¸«à¸²à¸Šà¸·à¹ˆà¸­ (NER Extraction)
        # à¸•à¸±à¸”à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸¡à¸²à¹à¸„à¹ˆ 5000 à¸•à¸±à¸§à¹à¸£à¸à¹€à¸à¸·à¹ˆà¸­à¸«à¸²à¸Šà¸·à¹ˆà¸­à¸à¹ˆà¸­à¸™ (à¹€à¸à¸·à¹ˆà¸­à¸„à¸§à¸²à¸¡à¹€à¸£à¹‡à¸§) à¸«à¸£à¸·à¸­à¸ˆà¸°à¸ªà¹ˆà¸‡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸à¹‡à¹„à¸”à¹‰à¸–à¹‰à¸²à¸£à¸­à¹„à¸«à¸§
        # à¹ƒà¸™à¸—à¸µà¹ˆà¸™à¸µà¹‰à¸ªà¹ˆà¸‡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¹à¸•à¹ˆà¸­à¸²à¸ˆà¸ˆà¸°à¸Šà¹‰à¸²à¸«à¸™à¹ˆà¸­à¸¢à¸ªà¸³à¸«à¸£à¸±à¸šà¸™à¸´à¸¢à¸²à¸¢à¸¢à¸²à¸§à¸¡à¸²à¸à¹†
        found_entities = extract_entities(novel_text)
        
        # à¸£à¸§à¸¡à¸Šà¸·à¹ˆà¸­à¸—à¸µà¹ˆ AI à¹€à¸ˆà¸­ + à¸Šà¸·à¹ˆà¸­à¸—à¸µà¹ˆà¸„à¸™à¸à¸´à¸¡à¸à¹Œà¹€à¸à¸´à¹ˆà¸¡
        auto_chars = list(set(found_entities["PERSON"])) # à¸•à¸±à¸”à¸„à¸³à¸‹à¹‰à¸³
        user_chars = [c.strip() for c in manual_chars.split(",") if c.strip() != ""]
        final_char_list = list(set(auto_chars + user_chars))
        
        # à¸à¸£à¸­à¸‡à¸Šà¸·à¹ˆà¸­à¸ªà¸±à¹‰à¸™à¹† à¸—à¸´à¹‰à¸‡ (à¹€à¸Šà¹ˆà¸™ "à¸™à¸²" "à¸") à¸›à¹‰à¸­à¸‡à¸à¸±à¸™à¸‚à¸¢à¸°
        final_char_list = [c for c in final_char_list if len(c) > 1]

        # --- à¸ªà¹ˆà¸§à¸™à¹à¸ªà¸”à¸‡à¸œà¸¥ ---
        st.divider()
        st.success(f"âœ… à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™! à¸à¸šà¸•à¸±à¸§à¸¥à¸°à¸„à¸£à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” {len(final_char_list)} à¸„à¸™")

        # à¸ªà¸£à¹‰à¸²à¸‡ Tabs
        tab1, tab2, tab3, tab4, tab5 = st.tabs(["ğŸ—ºï¸ à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆ AI à¸à¸š", "ğŸ“Š à¸ªà¸–à¸´à¸•à¸´à¸à¸·à¹‰à¸™à¸à¸²à¸™", "ğŸ“ˆ à¸à¸£à¸²à¸Ÿà¸­à¸²à¸£à¸¡à¸“à¹Œ", "ğŸ•¸ï¸ à¸„à¸§à¸²à¸¡à¸ªà¸±à¸¡à¸à¸±à¸™à¸˜à¹Œ", "ğŸ“ à¸„à¸³à¸‹à¹‰à¸³"])

        # === TAB 1: AI Discovery ===
        with tab1:
            st.header("ğŸ” à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆ AI à¸„à¹‰à¸™à¹€à¸ˆà¸­à¹ƒà¸™à¹€à¸£à¸·à¹ˆà¸­à¸‡ (Named Entities)")
            col_ent1, col_ent2, col_ent3 = st.columns(3)
            
            with col_ent1:
                st.info(f"ğŸ‘¤ à¸•à¸±à¸§à¸¥à¸°à¸„à¸£/à¸Šà¸·à¹ˆà¸­à¸„à¸™ ({len(set(found_entities['PERSON']))})")
                st.write(", ".join(set(found_entities["PERSON"])))
                
            with col_ent2:
                st.warning(f"ğŸ“ à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆ ({len(set(found_entities['LOCATION']))})")
                st.write(", ".join(set(found_entities["LOCATION"])))
                
            with col_ent3:
                st.success(f"ğŸ“… à¸§à¸±à¸™à¹à¸¥à¸°à¹€à¸§à¸¥à¸² ({len(set(found_entities['DATE'] + found_entities['TIME']))})")
                st.write(", ".join(set(found_entities['DATE'] + found_entities['TIME'])))

        # === TAB 2: Basic Stats ===
        with tab2:
            st.header("à¸ªà¸–à¸´à¸•à¸´à¸ à¸²à¸à¸£à¸§à¸¡")
            n_words = len(words)
            read_time = round(n_words / 200)
            vocab = set(words)
            diversity = round((len(vocab) / n_words) * 100, 2)
            
            c1, c2, c3 = st.columns(3)
            c1.metric("à¸ˆà¸³à¸™à¸§à¸™à¸„à¸³à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”", f"{n_words:,}")

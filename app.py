import streamlit as st
import pandas as pd
import altair as alt
from pythainlp import word_tokenize
from pythainlp.util import isthai
from pythainlp.tag import NER
from collections import Counter
import graphviz
import io

# --- р╕Хр╕▒р╣Йр╕Зр╕Др╣Ир╕▓р╕лр╕Щр╣Йр╕▓р╣Ар╕зр╣Зр╕Ъ ---
st.set_page_config(page_title="Pro Novel Analyst AI", page_icon="ЁЯдЦ", layout="wide")

st.title("ЁЯдЦ Pro Novel Analyst: р╕гр╕░р╕Ър╕Ър╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╕Щр╕┤р╕вр╕▓р╕вр╕нр╕▒р╕Ир╕Йр╕гр╕┤р╕вр╕░")
st.info("р╕нр╕▒р╕Ыр╣Ар╕Бр╕гр╕Фр╣Гр╕лр╕бр╣И! р╕гр╕нр╕Зр╕гр╕▒р╕Ър╕Бр╕▓р╕гр╕нр╕▒р╕Ыр╣Вр╕лр╕ер╕Фр╣Др╕Яр╕ер╣М р╣Бр╕ер╕░р╕Др╣Йр╕Щр╕лр╕▓р╕Хр╕▒р╕зр╕ер╕░р╕Др╕г/р╕кр╕Цр╕▓р╕Щр╕Чр╕╡р╣Ир╣Гр╕лр╣Йр╣Ар╕нр╕Зр╕нр╕▒р╕Хр╣Вр╕Щр╕бр╕▒р╕Хр╕┤")

# --- р╕кр╣Ир╕зр╕Щр╕гр╕▒р╕Ър╕Вр╣Йр╕нр╕бр╕╣р╕е ---
col_left, col_right = st.columns([1, 1])

with col_left:
    st.subheader("1. р╣Гр╕кр╣Ир╣Ар╕Щр╕╖р╣Йр╕нр╕лр╕▓р╕Щр╕┤р╕вр╕▓р╕в")
    input_method = st.radio("р╣Ар╕ер╕╖р╕нр╕Бр╕зр╕┤р╕Шр╕╡р╕Щр╕│р╣Ар╕Вр╣Йр╕▓р╕Вр╣Йр╕нр╕бр╕╣р╕е:", ["ЁЯУВ р╕нр╕▒р╕Ыр╣Вр╕лр╕ер╕Фр╣Др╕Яр╕ер╣М (.txt)", "тЬНя╕П р╕зр╕▓р╕Зр╕Вр╣Йр╕нр╕Др╕зр╕▓р╕бр╣Ар╕нр╕З"])
    
    novel_text = ""
    
    if input_method == "ЁЯУВ р╕нр╕▒р╕Ыр╣Вр╕лр╕ер╕Фр╣Др╕Яр╕ер╣М (.txt)":
        uploaded_file = st.file_uploader("р╣Ар╕ер╕╖р╕нр╕Бр╣Др╕Яр╕ер╣Мр╕Щр╕┤р╕вр╕▓р╕вр╕Вр╕нр╕Зр╕Др╕╕р╕У", type=['txt'])
        if uploaded_file is not None:
            stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
            novel_text = stringio.read()
            st.success(f"тЬЕ р╕нр╣Ир╕▓р╕Щр╣Др╕Яр╕ер╣Мр╕кр╕│р╣Ар╕гр╣Зр╕И! (р╕Др╕зр╕▓р╕бр╕вр╕▓р╕з {len(novel_text)} р╕Хр╕▒р╕зр╕нр╕▒р╕Бр╕йр╕г)")
            
    else:
        novel_text = st.text_area("р╕зр╕▓р╕Зр╣Ар╕Щр╕╖р╣Йр╕нр╕лр╕▓р╕Щр╕┤р╕вр╕▓р╕вр╕Чр╕╡р╣Ир╕Щр╕╡р╣И:", height=300)

with col_right:
    st.subheader("2. р╕Хр╕▒р╣Йр╕Зр╕Др╣Ир╕▓р╕Бр╕▓р╕гр╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣М")
    manual_chars = st.text_area("р╣Ар╕Юр╕┤р╣Ир╕бр╕Кр╕╖р╣Ир╕нр╕Хр╕▒р╕зр╕ер╕░р╕Др╕гр╣Ар╕нр╕З (р╕Др╕▒р╣Ир╕Щр╕Фр╣Йр╕зр╕вр╕Ир╕╕р╕ер╕ар╕▓р╕Д , )", placeholder="р╣Ар╕Кр╣Ир╕Щ: р╕кр╕бр╕Кр╕▓р╕в, р╕кр╕бр╕лр╕Нр╕┤р╕З", height=100)
    analyze_btn = st.button("ЁЯЪА р╕кр╕▒р╣Ир╕З AI р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╣Ар╕Фр╕╡р╣Лр╕вр╕зр╕Щр╕╡р╣Й!", type="primary", use_container_width=True)

# --- р╕Яр╕▒р╕Зр╕Бр╣Мр╕Кр╕▒р╕Щ AI (NER Engine) ---
@st.cache_resource
def load_ner_engine():
    return NER("thainer")

def extract_entities(text):
    ner = load_ner_engine()
    tags = ner.tag(text)
    
    entities = {
        "PERSON": [],
        "LOCATION": [],
        "DATE": [],
        "TIME": []
    }
    
    for item in tags:
        # р╕гр╕▒р╕Ър╕Др╣Ир╕▓р╣Бр╕Ър╕Ър╕вр╕╖р╕Фр╕лр╕вр╕╕р╣Ир╕Щ (р╕Ыр╣Йр╕нр╕Зр╕Бр╕▒р╕Щ Error ValueError)
        if len(item) == 3:
            word, pos, tag = item
        elif len(item) == 2:
            word, tag = item
        else:
            continue 
            
        if "PERSON" in tag:
            entities["PERSON"].append(word)
        elif "LOCATION" in tag:
            entities["LOCATION"].append(word)
        elif "DATE" in tag:
            entities["DATE"].append(word)
        elif "TIME" in tag:
            entities["TIME"].append(word)
            
    return entities

def analyze_sentiment(words):
    pos_words = ["р╕гр╕▒р╕Б", "р╕Фр╕╡", "р╕кр╕╕р╕В", "р╕кр╕зр╕в", "р╕вр╕┤р╣Йр╕б", "р╕лр╕▒р╕зр╣Ар╕гр╕▓р╕░", "р╕Кр╕нр╕Ъ", "р╕нр╕Ър╕нр╕╕р╣Ир╕Щ", "р╕лр╕зр╕▓р╕Щ", "р╕Хр╕╖р╣Ир╕Щр╣Ар╕Хр╣Йр╕Щ", "р╕кр╕│р╣Ар╕гр╣Зр╕И", "р╕гр╕нр╕Ф", "р╕Кр╕Щр╕░"]
    neg_words = ["р╣Ар╕Бр╕ер╕╡р╕вр╕Ф", "р╕Хр╕▓р╕в", "р╕Жр╣Ир╕▓", "р╣Ар╕ер╕з", "р╕гр╣Йр╕нр╕Зр╣Др╕лр╣Й", "р╣Ар╕Ир╣Зр╕Ъ", "р╣Вр╕Бр╕гр╕Ш", "р╣Ар╕ир╕гр╣Йр╕▓", "р╕Чр╕гр╕бр╕▓р╕Щ", "р╕Бр╕ер╕▒р╕з", "р╕бр╕╖р╕Фр╕бр╕Щ", "р╣Бр╕Юр╣Й", "р╣Ар╕Ир╣Зр╕Ър╕Ыр╕зр╕Ф"]
    
    score = 0
    if len(words) > 0:
        pos_cnt = sum(1 for w in words if w in pos_words)
        neg_cnt = sum(1 for w in words if w in neg_words)
        score = pos_cnt - neg_cnt
    return score

# --- р╣Ар╕гр╕┤р╣Ир╕бр╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╣Ар╕бр╕╖р╣Ир╕нр╕Бр╕Фр╕Ыр╕╕р╣Ир╕б ---
if analyze_btn and novel_text:
    with st.spinner('ЁЯдЦ AI р╕Бр╕│р╕ер╕▒р╕Зр╕Чр╕│р╕Зр╕▓р╕Щ... (р╕нр╣Ир╕▓р╕Щр╕Щр╕┤р╕вр╕▓р╕в > р╕лр╕▓р╕Хр╕▒р╕зр╕ер╕░р╕Др╕г > р╕кр╕гр╣Йр╕▓р╕Зр╕Бр╕гр╕▓р╕Я)'):
        
        # 1. р╕Хр╕▒р╕Фр╕Др╕│
        raw_words = word_tokenize(novel_text, engine="newmm")
        words = [w for w in raw_words if w.strip() != "" and isthai(w)]
        
        # 2. р╣Ар╕гр╕╡р╕вр╕Б AI р╕лр╕▓р╕Кр╕╖р╣Ир╕н
        found_entities = extract_entities(novel_text)
        
        # р╕гр╕зр╕бр╕Кр╕╖р╣Ир╕н
        auto_chars = list(set(found_entities["PERSON"]))
        user_chars = [c.strip() for c in manual_chars.split(",") if c.strip() != ""]
        final_char_list = list(set(auto_chars + user_chars))
        final_char_list = [c for c in final_char_list if len(c) > 1]

        # --- р╣Бр╕кр╕Фр╕Зр╕Ьр╕е ---
        st.divider()
        st.success(f"тЬЕ р╕зр╕┤р╣Ар╕Др╕гр╕▓р╕░р╕лр╣Мр╣Ар╕кр╕гр╣Зр╕Ир╕кр╕┤р╣Йр╕Щ! р╕Юр╕Ър╕Хр╕▒р╕зр╕ер╕░р╕Др╕гр╕Чр╕▒р╣Йр╕Зр╕лр╕бр╕Ф {len(final_char_list)} р╕Др╕Щ")

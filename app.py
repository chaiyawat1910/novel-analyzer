import streamlit as st
import pandas as pd
import altair as alt
from pythainlp import word_tokenize
from pythainlp.util import isthai
from pythainlp.tag import NER
from collections import Counter
import graphviz
import io

# --- ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏ß‡πá‡∏ö ---
st.set_page_config(page_title="Pro Novel Analyst AI", page_icon="ü§ñ", layout="wide")

# --- 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥ (Session State) ---
# ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡∏´‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à
if 'analyzed' not in st.session_state:
    st.session_state.analyzed = False
if 'result_data' not in st.session_state:
    st.session_state.result_data = {}

st.title("ü§ñ Pro Novel Analyst: ‡∏£‡∏∞‡∏ö‡∏ö‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏≠‡∏±‡∏à‡∏â‡∏£‡∏¥‡∏¢‡∏∞")

# --- ‡∏™‡πà‡∏ß‡∏ô‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• ---
col_left, col_right = st.columns([1, 1])

with col_left:
    st.subheader("1. ‡πÉ‡∏™‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢")
    input_method = st.radio("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏ß‡∏¥‡∏ò‡∏µ‡∏ô‡∏≥‡πÄ‡∏Ç‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•:", ["üìÇ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (.txt)", "‚úçÔ∏è ‡∏ß‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏≠‡∏á"])
    
    novel_text = ""
    
    if input_method == "üìÇ ‡∏≠‡∏±‡∏õ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå (.txt)":
        uploaded_file = st.file_uploader("‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì", type=['txt'])
        if uploaded_file is not None:
            stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
            novel_text = stringio.read()
            st.success(f"‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à! (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏¢‡∏≤‡∏ß {len(novel_text):,} ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£)")
            
    else:
        novel_text = st.text_area("‡∏ß‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏ó‡∏µ‡πà‡∏ô‡∏µ‡πà:", height=300)

with col_right:
    st.subheader("2. ‡∏ï‡∏±‡πâ‡∏á‡∏Ñ‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå")
    manual_chars = st.text_area("‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£‡πÄ‡∏≠‡∏á (‡∏Ñ‡∏±‡πà‡∏ô‡∏î‡πâ‡∏ß‡∏¢‡∏à‡∏∏‡∏•‡∏†‡∏≤‡∏Ñ)", placeholder="‡πÄ‡∏ä‡πà‡∏ô: ‡∏™‡∏°‡∏ä‡∏≤‡∏¢, ‡∏™‡∏°‡∏´‡∏ç‡∏¥‡∏á", height=100)
    
    # ‡∏õ‡∏∏‡πà‡∏°‡∏Å‡∏î‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≥
    if st.button("üöÄ ‡∏™‡∏±‡πà‡∏á AI ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏î‡∏µ‡πã‡∏¢‡∏ß‡∏ô‡∏µ‡πâ!", type="primary", use_container_width=True):
        if novel_text:
            st.session_state.analyzed = True
            st.rerun() # ‡∏™‡∏±‡πà‡∏á‡∏£‡∏µ‡πÄ‡∏ü‡∏£‡∏ä‡∏´‡∏ô‡πâ‡∏≤‡∏à‡∏≠‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
        else:
            st.error("‚ö†Ô∏è ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡πÉ‡∏™‡πà‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ô‡∏¥‡∏¢‡∏≤‡∏¢‡∏Å‡πà‡∏≠‡∏ô‡∏Ñ‡∏£‡∏±‡∏ö")

# --- ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô AI (NER Engine) ---
@st.cache_resource
def load_ner_engine():
    return NER("thainer")

def extract_entities(text):
    ner = load_ner_engine()
    # ‡∏ï‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏´‡πâ‡∏™‡∏±‡πâ‡∏ô‡∏•‡∏á‡∏´‡∏ô‡πà‡∏≠‡∏¢‡∏ñ‡πâ‡∏≤‡∏¢‡∏≤‡∏ß‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ (‡∏õ‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ô‡∏£‡∏≠‡∏ô‡∏≤‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô 5 ‡∏ô‡∏≤‡∏ó‡∏µ)
    # ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡πÑ‡∏´‡∏ß‡∏Å‡πá‡πÄ‡∏≠‡∏≤ limit ‡∏≠‡∏≠‡∏Å‡πÑ‡∏î‡πâ
    processed_text = text[:100000] if len(text) > 100000 else text 
    tags = ner.tag(processed_text)
    
    entities = {
        "PERSON": [],
        "LOCATION": [],
        "DATE": [],
        "TIME": []
    }
    
    # ‡∏Ñ‡∏≥‡∏™‡∏£‡∏£‡∏û‡∏ô‡∏≤‡∏°‡∏ó‡∏µ‡πà AI ‡∏°‡∏±‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ú‡∏¥‡∏î‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏ä‡∏∑‡πà‡∏≠‡∏Ñ‡∏ô (Blacklist)
    blacklist_names = [
        "‡πÄ‡∏Ç‡∏≤", "‡πÄ‡∏ò‡∏≠", "‡∏°‡∏±‡∏ô", "‡∏â‡∏±‡∏ô", "‡∏ú‡∏°", "‡∏Å‡∏π", "‡∏°‡∏∂‡∏á", "‡∏Ç‡πâ‡∏≤", "‡πÄ‡∏≠‡πá‡∏á", "‡πÄ‡∏£‡∏≤", "‡∏û‡∏ß‡∏Å‡πÄ‡∏£‡∏≤",
        "‡∏û‡∏µ‡πà", "‡∏ô‡πâ‡∏≠‡∏á", "‡∏•‡∏∏‡∏á", "‡∏õ‡πâ‡∏≤", "‡∏ô‡πâ‡∏≤", "‡∏≠‡∏≤", "‡∏û‡πà‡∏≠", "‡πÅ‡∏°‡πà", "‡∏õ‡∏π‡πà", "‡∏¢‡πà‡∏≤", "‡∏ï‡∏≤", "‡∏¢‡∏≤‡∏¢",
        "‡∏Ñ‡∏∏‡∏ì", "‡∏ó‡πà‡∏≤‡∏ô", "‡πÅ‡∏Å", "‡πÉ‡∏Ñ‡∏£", "‡∏ô‡∏≤‡∏á", "‡∏ô‡∏≤‡∏¢", "‡πÄ‡∏î‡πá‡∏Å", "‡∏ú‡∏π‡πâ‡∏ä‡∏≤‡∏¢", "‡∏ú‡∏π‡πâ‡∏´‡∏ç‡∏¥‡∏á", "‡∏Ñ‡∏ô",
        "‡∏ï‡∏±‡∏ß‡πÄ‡∏≠‡∏á", "‡∏ö‡πà‡∏≤‡∏ß", "‡∏ù‡πà‡∏≤‡∏ö‡∏≤‡∏ó", "‡∏û‡∏£‡∏∞‡∏≠‡∏á‡∏Ñ‡πå", "‡∏´‡∏°‡∏≠", "‡∏Ñ‡∏£‡∏π", "‡∏≠‡∏≤‡∏à‡∏≤‡∏£‡∏¢‡πå"
    ]

    for item in tags:
        if len(item) == 3:
            word, pos, tag = item
        elif len(item) == 2:
            word, tag = item
        else:
            continue 
            
        word_clean = word.strip()
        
        if "PERSON" in tag:
            # ‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏±‡πâ‡∏ô‡πÄ‡∏Å‡∏¥‡∏ô 1 ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£ ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô Blacklist
            if len(word_clean) > 1 and word_clean not in blacklist_names:
                entities["PERSON"].append(word_clean)
        elif "LOCATION" in tag:
            entities["LOCATION"].append(word_clean)
        elif "DATE" in tag:
            entities["DATE"].append(word_clean)
        elif "TIME" in tag:
            entities["TIME"].append(word_clean)
            
    return entities

def analyze_sentiment(words):
    pos_words = ["‡∏£‡∏±‡∏Å", "‡∏î‡∏µ", "‡∏™‡∏∏‡∏Ç", "‡∏™‡∏ß‡∏¢", "‡∏¢‡∏¥‡πâ‡∏°", "‡∏´‡∏±‡∏ß‡πÄ‡∏£‡∏≤‡∏∞", "‡∏ä‡∏≠‡∏ö", "‡∏≠‡∏ö‡∏≠‡∏∏‡πà‡∏ô", "‡∏´‡∏ß‡∏≤‡∏ô", "‡∏ï‡∏∑‡πà‡∏ô‡πÄ‡∏ï‡πâ‡∏ô", "‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à", "‡∏£‡∏≠‡∏î", "‡∏ä‡∏ô‡∏∞"]
    neg_words = ["‡πÄ‡∏Å‡∏•‡∏µ‡∏¢‡∏î", "‡∏ï‡∏≤‡∏¢", "‡∏Ü‡πà‡∏≤", "‡πÄ‡∏•‡∏ß", "‡∏£‡πâ‡∏≠‡∏á‡πÑ‡∏´‡πâ", "‡πÄ‡∏à‡πá‡∏ö", "‡πÇ‡∏Å‡∏£‡∏ò", "‡πÄ‡∏®‡∏£‡πâ‡∏≤", "‡∏ó‡∏£‡∏°‡∏≤‡∏ô", "‡∏Å‡∏•‡∏±‡∏ß", "‡∏°‡∏∑‡∏î‡∏°‡∏ô", "‡πÅ‡∏û‡πâ", "‡πÄ‡∏à‡πá‡∏ö‡∏õ‡∏ß‡∏î"]
    score = 0
    if len(words) > 0:
        pos_cnt = sum(1 for w in words if w in pos_words)
        neg_cnt = sum(1 for w in words if w in neg_words)
        score = pos_cnt - neg_cnt
    return score

# --- ‡∏™‡πà‡∏ß‡∏ô‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• (‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏™‡∏ñ‡∏≤‡∏ô‡∏∞ analyzed ‡πÄ‡∏õ‡πá‡∏ô True) ---
if st.session_state.analyzed and novel_text:
    
    st.divider()
    
    with st.spinner('ü§ñ AI ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ó‡∏≥‡∏á‡∏≤‡∏ô... (‡∏Å‡∏£‡∏≠‡∏á‡∏ä‡∏∑‡πà‡∏≠‡∏ã‡πâ‡∏≥ + ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü)'):
        
        # 1. ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥
        raw_words = word_tokenize(novel_text, engine="newmm")
        words = [w for w in raw_words if w.strip() != "" and isthai(w)]
        
        # 2. ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å AI ‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠
        found_entities = extract_entities(novel_text)
        
        # 3. ‡∏£‡∏ß‡∏°‡∏ä‡∏∑‡πà‡∏≠‡πÅ‡∏•‡∏∞‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥ (‡πÉ‡∏ä‡πâ set ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ä‡∏∑‡πà‡∏≠‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏ã‡πâ‡∏≥‡∏Å‡∏±‡∏ô)
        auto_chars = list(set(found_entities["PERSON"])) 
        user_chars = [c.strip() for c in manual_chars.split(",") if c.strip() != ""]
        
        # ‡∏£‡∏ß‡∏° + ‡∏ï‡∏±‡∏î‡∏ã‡πâ‡∏≥‡∏≠‡∏µ‡∏Å‡∏£‡∏≠‡∏ö
        final_char_list = list(set(auto_chars + user_chars))
        final_char_list.sort() # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏ï‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£

        st.success(f"‚úÖ ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡πÄ‡∏™‡∏£‡πá‡∏à‡∏™‡∏¥‡πâ‡∏ô! ‡∏û‡∏ö‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£ (Unique) ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(final_char_list)} ‡∏Ñ‡∏ô")

        # --- ‡∏™‡∏£‡πâ‡∏≤‡∏á Tabs ---
        tab1, tab2, tab3, tab4, tab5 = st.tabs(["üó∫Ô∏è ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà AI ‡∏û‡∏ö", "üìä ‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô", "üìà ‡∏Å‡∏£‡∏≤‡∏ü‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå", "üï∏Ô∏è ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå", "üìù ‡∏Ñ‡∏≥‡∏ã‡πâ‡∏≥"])

        # TAB 1: AI Findings
        with tab1:
            st.header("üîç ‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà AI ‡∏Ñ‡πâ‡∏ô‡πÄ‡∏à‡∏≠")
            st.info("üí° ‡∏´‡∏°‡∏≤‡∏¢‡πÄ‡∏´‡∏ï‡∏∏: ‡∏£‡∏∞‡∏ö‡∏ö‡∏Å‡∏£‡∏≠‡∏á‡∏Ñ‡∏≥‡∏™‡∏£‡∏£‡∏û‡∏ô‡∏≤‡∏° (‡πÄ‡∏Ç‡∏≤, ‡πÄ‡∏ò‡∏≠, ‡∏â‡∏±‡∏ô) ‡∏≠‡∏≠‡∏Å‡πÉ‡∏´‡πâ‡πÅ‡∏•‡πâ‡∏ß")
            
            c1, c2, c3 = st.columns(3)
            with c1:
                st.write(f"**üë§ ‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£ ({len(final_char_list)})**")
                # ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏õ‡πá‡∏ô Chip ‡∏™‡∏ß‡∏¢‡πÜ
                st.write(", ".join(final_char_list))
            with c2:
                locs = list(set(found_entities['LOCATION']))
                st.write(f"**üìç ‡∏™‡∏ñ‡∏≤‡∏ô‡∏ó‡∏µ‡πà ({len(locs)})**")
                st.write(", ".join(locs))
            with c3:
                times = list(set(found_entities['DATE'] + found_entities['TIME']))
                st.write(f"**üìÖ ‡πÄ‡∏ß‡∏•‡∏≤ ({len(times)})**")
                st.write(", ".join(times))

        # TAB 2: Stats
        with tab2:
            st.header("‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏†‡∏≤‡∏û‡∏£‡∏ß‡∏°")
            n_words = len(words)
            read_time = round(n_words / 200)
            vocab = set(words)
            diversity = round((len(vocab) / n_words) * 100, 2)
            c1, c2, c3 = st.columns(3)
            c1.metric("‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏≥‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î", f"{n_words:,}")
            c2.metric("‡πÄ‡∏ß‡∏•‡∏≤‡∏≠‡πà‡∏≤‡∏ô (‡∏ô‡∏≤‡∏ó‡∏µ)", read_time)
            c3.metric("‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏•‡∏≤‡∏Å‡∏´‡∏•‡∏≤‡∏¢‡∏Ñ‡∏≥", f"{diversity}%")

        # TAB 3: Sentiment
        with tab3:
            st.header("‡∏Å‡∏£‡∏≤‡∏ü‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå")
            chunk_size = 500 # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏ô‡∏≤‡∏î chunk ‡πÉ‡∏´‡πâ‡∏≠‡πà‡∏≤‡∏ô‡∏Å‡∏£‡∏≤‡∏ü‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô
            chunks = [words[i:i + chunk_size] for i in range(0, len(words), chunk_size)]
            sentiment_scores = [analyze_sentiment(chunk) for chunk in chunks]
            chart_data = pd.DataFrame({'Position': range(len(sentiment_scores)), 'Score': sentiment_scores})
            line_chart = alt.Chart(chart_data).mark_line(interpolate='basis').encode(
                x=alt.X('Position', title='‡∏ä‡πà‡∏ß‡∏á‡πÄ‡∏ß‡∏•‡∏≤‡∏Ç‡∏≠‡∏á‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á'),
                y=alt.Y('Score', title='‡∏Ñ‡∏∞‡πÅ‡∏ô‡∏ô‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå'),
                color=alt.value("#FF4B4B"),
                tooltip=['Position', 'Score']
            ).properties(height=350)
            st.altair_chart(line_chart, use_container_width=True)

        # TAB 4: Network
        with tab4:
            st.header("‡πÄ‡∏Ñ‡∏£‡∏∑‡∏≠‡∏Ç‡πà‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå")
            if not final_char_list:
                st.warning("‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£")
            else:
                # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£ Top 15 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÄ‡∏à‡∏≠‡∏ö‡πà‡∏≠‡∏¢‡∏™‡∏∏‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏≤‡∏ü‡∏£‡∏Å‡∏à‡∏ô‡∏î‡∏π‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á
                # ‡∏ô‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ñ‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠‡πÉ‡∏ô‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤
                char_freq = {name: novel_text.count(name) for name in final_char_list}
                # ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö‡πÄ‡∏≠‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞ Top 15
                top_chars = sorted(char_freq, key=char_freq.get, reverse=True)[:15]
                
                graph = graphviz.Digraph()
                graph.attr(rankdir='LR')
                
                # Logic ‡πÄ‡∏î‡∏¥‡∏° ‡πÅ‡∏ï‡πà‡πÉ‡∏ä‡πâ Top Chars
                paragraphs = novel_text.split('\n')
                relations = Counter()
                
                for para in paragraphs:
                    # ‡∏´‡∏≤‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£ Top 15 ‡πÉ‡∏ô‡∏¢‡πà‡∏≠‡∏´‡∏ô‡πâ‡∏≤‡∏ô‡∏±‡πâ‡∏ô
                    found_in_para = [c for c in top_chars if c in para]
                    if len(found_in_para) > 1:
                        for i in range(len(found_in_para)):
                            for j in range(i+1, len(found_in_para)):
                                pair = tuple(sorted([found_in_para[i], found_in_para[j]]))
                                relations[pair] += 1
                                
                # ‡∏ß‡∏≤‡∏î‡∏Å‡∏£‡∏≤‡∏ü
                for char in top_chars:
                    graph.node(char, style='filled', fillcolor='#D3D3D3')
                    
                for (char1, char2), weight in relations.items():
                    if weight > 0:
                        # ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡πÄ‡∏™‡πâ‡∏ô‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡∏´‡∏ô‡∏≤‡πÄ‡∏Å‡∏¥‡∏ô‡πÑ‡∏õ
                        pen_width = min(weight/2, 5) 
                        graph.edge(char1, char2, penwidth=str(pen_width), label=str(weight))
                        
                st.graphviz_chart(graph)
                st.caption(f"‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡∏ï‡∏±‡∏ß‡∏•‡∏∞‡∏Ñ‡∏£‡∏´‡∏•‡∏±‡∏Å 15 ‡∏ï‡∏±‡∏ß‡πÅ‡∏£‡∏Å (‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î {len(final_char_list)} ‡∏ï‡∏±‡∏ß) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏ß‡∏¢‡∏á‡∏≤‡∏°")

        # TAB 5: Word Cloud
        with tab5:
            st.header("‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏ö‡πà‡∏≠‡∏¢")
            word_counts = Counter(words)
            df_words = pd.DataFrame(word_counts.most_common(20), columns=['‡∏Ñ‡∏≥‡∏®‡∏±‡∏û‡∏ó‡πå', '‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ñ‡∏£‡∏±‡πâ‡∏á'])
            st.dataframe(df_words, use_container_width=True)

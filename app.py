import streamlit as st
import pandas as pd
import altair as alt
from pythainlp import word_tokenize
from pythainlp.util import isthai
from pythainlp.tag import NER
from collections import Counter
import graphviz
import io

# --- à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¸«à¸™à¹‰à¸²à¹€à¸§à¹‡à¸š ---
st.set_page_config(page_title="Pro Novel Analyst AI", page_icon="ðŸ¤–", layout="wide")

st.title("ðŸ¤– Pro Novel Analyst: à¸£à¸°à¸šà¸šà¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¸™à¸´à¸¢à¸²à¸¢à¸­à¸±à¸ˆà¸‰à¸£à¸´à¸¢à¸°")
st.info("à¸­à¸±à¸›à¹€à¸à¸£à¸”à¹ƒà¸«à¸¡à¹ˆ! à¸£à¸­à¸‡à¸£à¸±à¸šà¸à¸²à¸£à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ à¹à¸¥à¸°à¸„à¹‰à¸™à¸«à¸²à¸•à¸±à¸§à¸¥à¸°à¸„à¸£/à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆà¹ƒà¸«à¹‰à¹€à¸­à¸‡à¸­à¸±à¸•à¹‚à¸™à¸¡à¸±à¸•à¸´")

# --- à¸ªà¹ˆà¸§à¸™à¸£à¸±à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---
col_left, col_right = st.columns([1, 1])

with col_left:
    st.subheader("1. à¹ƒà¸ªà¹ˆà¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸™à¸´à¸¢à¸²à¸¢")
    input_method = st.radio("à¹€à¸¥à¸·à¸­à¸à¸§à¸´à¸˜à¸µà¸™à¸³à¹€à¸‚à¹‰à¸²à¸‚à¹‰à¸­à¸¡à¸¹à¸¥:", ["ðŸ“‚ à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ (.txt)", "âœï¸ à¸§à¸²à¸‡à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹€à¸­à¸‡"])
    
    novel_text = ""
    
    if input_method == "ðŸ“‚ à¸­à¸±à¸›à¹‚à¸«à¸¥à¸”à¹„à¸Ÿà¸¥à¹Œ (.txt)":
        uploaded_file = st.file_uploader("à¹€à¸¥à¸·à¸­à¸à¹„à¸Ÿà¸¥à¹Œà¸™à¸´à¸¢à¸²à¸¢à¸‚à¸­à¸‡à¸„à¸¸à¸“", type=['txt'])
        if uploaded_file is not None:
            stringio = io.StringIO(uploaded_file.getvalue().decode("utf-8"))
            novel_text = stringio.read()
            st.success(f"âœ… à¸­à¹ˆà¸²à¸™à¹„à¸Ÿà¸¥à¹Œà¸ªà¸³à¹€à¸£à¹‡à¸ˆ! (à¸„à¸§à¸²à¸¡à¸¢à¸²à¸§ {len(novel_text)} à¸•à¸±à¸§à¸­à¸±à¸à¸©à¸£)")
            
    else:
        novel_text = st.text_area("à¸§à¸²à¸‡à¹€à¸™à¸·à¹‰à¸­à¸«à¸²à¸™à¸´à¸¢à¸²à¸¢à¸—à¸µà¹ˆà¸™à¸µà¹ˆ:", height=300)

with col_right:
    st.subheader("2. à¸•à¸±à¹‰à¸‡à¸„à¹ˆà¸²à¸à¸²à¸£à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œ")
    manual_chars = st.text_area("à¹€à¸žà¸´à¹ˆà¸¡à¸Šà¸·à¹ˆà¸­à¸•à¸±à¸§à¸¥à¸°à¸„à¸£à¹€à¸­à¸‡ (à¸„à¸±à¹ˆà¸™à¸”à¹‰à¸§à¸¢à¸ˆà¸¸à¸¥à¸ à¸²à¸„ , )", placeholder="à¹€à¸Šà¹ˆà¸™: à¸ªà¸¡à¸Šà¸²à¸¢, à¸ªà¸¡à¸«à¸à¸´à¸‡", height=100)
    analyze_btn = st.button("ðŸš€ à¸ªà¸±à¹ˆà¸‡ AI à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸”à¸µà¹‹à¸¢à¸§à¸™à¸µà¹‰!", type="primary", use_container_width=True)

# --- à¸Ÿà¸±à¸‡à¸à¹Œà¸Šà¸±à¸™ AI (NER Engine) ---
@st.cache_resource
def load_ner_engine():
    return NER("thainer")

def extract_entities(text):
    ner = load_ner_engine()
    tags = ner.tag(text)
    
    entities = {
        "PERSON": [],
        "LOCATION": [],
        "DATE": [],
        "TIME": []
    }
    
    # --- à¸ˆà¸¸à¸”à¸—à¸µà¹ˆà¹à¸à¹‰à¹„à¸‚: à¹€à¸Šà¹‡à¸„à¸ˆà¸³à¸™à¸§à¸™à¸„à¹ˆà¸²à¸—à¸µà¹ˆà¸ªà¹ˆà¸‡à¸à¸¥à¸±à¸šà¸¡à¸²à¸à¹ˆà¸­à¸™à¸”à¸¶à¸‡à¸‚à¹‰à¸­à¸¡à¸¹à¸¥ ---
    for item in tags:
        # à¸£à¸±à¸šà¸„à¹ˆà¸²à¹à¸šà¸šà¸¢à¸·à¸”à¸«à¸¢à¸¸à¹ˆà¸™ (à¸›à¹‰à¸­à¸‡à¸à¸±à¸™ Error)
        if len(item) == 3:
            word, pos, tag = item
        elif len(item) == 2:
            word, tag = item
        else:
            continue # à¸–à¹‰à¸²à¸¡à¸²à¹à¸›à¸¥à¸à¹† à¹ƒà¸«à¹‰à¸‚à¹‰à¸²à¸¡à¹„à¸›
            
        # à¹€à¸à¹‡à¸šà¸‚à¹‰à¸­à¸¡à¸¹à¸¥à¸•à¸²à¸¡ Tag
        if "PERSON" in tag:
            entities["PERSON"].append(word)
        elif "LOCATION" in tag:
            entities["LOCATION"].append(word)
        elif "DATE" in tag:
            entities["DATE"].append(word)
        elif "TIME" in tag:
            entities["TIME"].append(word)
            
    return entities

def analyze_sentiment(words):
    pos_words = ["à¸£à¸±à¸", "à¸”à¸µ", "à¸ªà¸¸à¸‚", "à¸ªà¸§à¸¢", "à¸¢à¸´à¹‰à¸¡", "à¸«à¸±à¸§à¹€à¸£à¸²à¸°", "à¸Šà¸­à¸š", "à¸­à¸šà¸­à¸¸à¹ˆà¸™", "à¸«à¸§à¸²à¸™", "à¸•à¸·à¹ˆà¸™à¹€à¸•à¹‰à¸™", "à¸ªà¸³à¹€à¸£à¹‡à¸ˆ", "à¸£à¸­à¸”", "à¸Šà¸™à¸°"]
    # à¹à¸à¹‰à¹„à¸‚à¸šà¸£à¸£à¸—à¸±à¸”à¸™à¸µà¹‰à¸—à¸µà¹ˆ error à¸„à¸£à¸±à¸š (à¹€à¸•à¸´à¸¡à¹€à¸„à¸£à¸·à¹ˆà¸­à¸‡à¸«à¸¡à¸²à¸¢à¸›à¸´à¸”à¹ƒà¸«à¹‰à¸„à¸£à¸š)
    neg_words = ["à¹€à¸à¸¥à¸µà¸¢à¸”", "à¸•à¸²à¸¢", "à¸†à¹ˆà¸²", "à¹€à¸¥à¸§", "à¸£à¹‰à¸­à¸‡à¹„à¸«à¹‰", "à¹€à¸ˆà¹‡à¸š", "à¹‚à¸à¸£à¸˜", "à¹€à¸¨à¸£à¹‰à¸²", "à¸—à¸£à¸¡à¸²à¸™", "à¸à¸¥à¸±à¸§", "à¸¡à¸·à¸”à¸¡à¸™", "à¹à¸žà¹‰", "à¹€à¸ˆà¹‡à¸šà¸›à¸§à¸”"]
    
    score = 0
    if len(words) > 0:
        pos_cnt = sum(1 for w in words if w in pos_words)
        neg_cnt = sum(1 for w in words if w in neg_words)
        score = pos_cnt - neg_cnt
    return score

# --- à¹€à¸£à¸´à¹ˆà¸¡à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸¡à¸·à¹ˆà¸­à¸à¸”à¸›à¸¸à¹ˆà¸¡ ---
if analyze_btn and novel_text:
    with st.spinner('ðŸ¤– AI à¸à¸³à¸¥à¸±à¸‡à¸—à¸³à¸‡à¸²à¸™... (à¸­à¹ˆà¸²à¸™à¸™à¸´à¸¢à¸²à¸¢ > à¸«à¸²à¸•à¸±à¸§à¸¥à¸°à¸„à¸£ > à¸ªà¸£à¹‰à¸²à¸‡à¸à¸£à¸²à¸Ÿ)'):
        
        # 1. à¸•à¸±à¸”à¸„à¸³
        raw_words = word_tokenize(novel_text, engine="newmm")
        words = [w for w in raw_words if w.strip() != "" and isthai(w)]
        
        # 2. à¹€à¸£à¸µà¸¢à¸ AI à¸«à¸²à¸Šà¸·à¹ˆà¸­
        found_entities = extract_entities(novel_text)
        
        # à¸£à¸§à¸¡à¸Šà¸·à¹ˆà¸­
        auto_chars = list(set(found_entities["PERSON"]))
        user_chars = [c.strip() for c in manual_chars.split(",") if c.strip() != ""]
        final_char_list = list(set(auto_chars + user_chars))
        final_char_list = [c for c in final_char_list if len(c) > 1]

        # --- à¹à¸ªà¸”à¸‡à¸œà¸¥ ---
        st.divider()
        st.success(f"âœ… à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹€à¸ªà¸£à¹‡à¸ˆà¸ªà¸´à¹‰à¸™! à¸žà¸šà¸•à¸±à¸§à¸¥à¸°à¸„à¸£à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸” {len(final_char_list)} à¸„à¸™")

        tab1, tab2, tab3, tab4, tab5 = st.tabs(["ðŸ—ºï¸ à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆ AI à¸žà¸š", "ðŸ“Š à¸ªà¸–à¸´à¸•à¸´à¸žà¸·à¹‰à¸™à¸à¸²à¸™", "ðŸ“ˆ à¸à¸£à¸²à¸Ÿà¸­à¸²à¸£à¸¡à¸“à¹Œ", "ðŸ•¸ï¸ à¸„à¸§à¸²à¸¡à¸ªà¸±à¸¡à¸žà¸±à¸™à¸˜à¹Œ", "ðŸ“ à¸„à¸³à¸‹à¹‰à¸³"])

        # TAB 1
        with tab1:
            st.header("ðŸ” à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆ AI à¸„à¹‰à¸™à¹€à¸ˆà¸­")
            c1, c2, c3 = st.columns(3)
            with c1:
                st.info(f"ðŸ‘¤ à¸•à¸±à¸§à¸¥à¸°à¸„à¸£ ({len(set(found_entities['PERSON']))})")
                st.write(", ".join(set(found_entities['PERSON'])))
            with c2:
                st.warning(f"ðŸ“ à¸ªà¸–à¸²à¸™à¸—à¸µà¹ˆ ({len(set(found_entities['LOCATION']))})")
                st.write(", ".join(set(found_entities['LOCATION'])))
            with c3:
                st.success(f"ðŸ“… à¹€à¸§à¸¥à¸² ({len(set(found_entities['DATE'] + found_entities['TIME']))})")
                st.write(", ".join(set(found_entities['DATE'] + found_entities['TIME'])))

        # TAB 2
        with tab2:
            st.header("à¸ªà¸–à¸´à¸•à¸´à¸ à¸²à¸žà¸£à¸§à¸¡")
            n_words = len(words)
            read_time = round(n_words / 200)
            vocab = set(words)
            diversity = round((len(vocab) / n_words) * 100, 2)
            c1, c2, c3 = st.columns(3)
            c1.metric("à¸ˆà¸³à¸™à¸§à¸™à¸„à¸³à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”", f"{n_words:,}")
            c2.metric("à¹€à¸§à¸¥à¸²à¸­à¹ˆà¸²à¸™ (à¸™à¸²à¸—à¸µ)", read_time)
            c3.metric("à¸„à¸§à¸²à¸¡à¸«à¸¥à¸²à¸à¸«à¸¥à¸²à¸¢à¸„à¸³", f"{diversity}%")

        # TAB 3
        with tab3:
            st.header("à¸à¸£à¸²à¸Ÿà¸­à¸²à¸£à¸¡à¸“à¹Œ")
            chunk_size = 100
            chunks = [words[i:i + chunk_size] for i in range(0, len(words), chunk_size)]
            sentiment_scores = [analyze_sentiment(chunk) for chunk in chunks]
            chart_data = pd.DataFrame({'Position': range(len(sentiment_scores)), 'Score': sentiment_scores})
            line_chart = alt.Chart(chart_data).mark_line(interpolate='basis').encode(
                x='Position', y='Score', color=alt.value("#FF4B4B")
            ).properties(height=300)
            st.altair_chart(line_chart, use_container_width=True)

        # TAB 4
        with tab4:
            st.header("à¹€à¸„à¸£à¸·à¸­à¸‚à¹ˆà¸²à¸¢à¸„à¸§à¸²à¸¡à¸ªà¸±à¸¡à¸žà¸±à¸™à¸˜à¹Œ")
            if not final_char_list:
                st.warning("à¹„à¸¡à¹ˆà¸žà¸šà¸•à¸±à¸§à¸¥à¸°à¸„à¸£ à¸¥à¸­à¸‡à¸žà¸´à¸¡à¸žà¹Œà¹€à¸žà¸´à¹ˆà¸¡à¹€à¸­à¸‡")
            else:
                graph = graphviz.Digraph()
                graph.attr(rankdir='LR')
                paragraphs = novel_text.split('\n')
                relations = Counter()
                for para in paragraphs:
                    found_in_para = [c for c in final_char_list if c in para]
                    if len(found_in_para) > 1:
                        for i in range(len(found_in_para)):
                            for j in range(i+1, len(found_in_para)):
                                pair = tuple(sorted([found_in_para[i], found_in_para[j]]))
                                relations[pair] += 1
                for (char1, char2), weight in relations.items():
                    if weight > 0:
                        graph.edge(char1, char2, penwidth=str(weight/2), label=str(weight))
                        graph.node(char1, style='filled', fillcolor='#D3D3D3')
                        graph.node(char2, style='filled', fillcolor='#D3D3D3')
                st.graphviz_chart(graph)

        # TAB 5
        with tab5:
            st.header("à¸„à¸³à¸—à¸µà¹ˆà¹ƒà¸Šà¹‰à¸šà¹ˆà¸­à¸¢")
            word_counts = Counter(words)
            df_words = pd.DataFrame(word_counts.most_common(20), columns=['à¸„à¸³à¸¨à¸±à¸žà¸—à¹Œ', 'à¸ˆà¸³à¸™à¸§à¸™à¸„à¸£à¸±à¹‰à¸‡'])
            st.dataframe(df_words, use_
